<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Thao Pham</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="../favicon.ico">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="apple-touch-icon" href="../apple-touch-icon.png">
</head>
<body>
    <div class="container">
        <!-- Navigation -->
        <nav>
            <div class="nav-brand">
                <a href="/">Thao Pham</a>
            </div>
            <div class="nav-right">
                <ul class="nav-links">
                    <li><a href="/research/" class="active">Research</a></li>
                    <li><a href="/blog/">Blog</a></li>
                    <li><a href="/other/">Other</a></li>
                </ul>
                <button id="theme-toggle" class="theme-toggle">
                    <span class="sun">‚òÄÔ∏è</span>
                    <span class="moon">üåô</span>
                </button>
            </div>
        </nav>

        <!-- Page Content -->
        <main class="publications-main">
            <div class="publications-header">
                <h1>Research</h1>
                <p class="publications-subtitle">Selected publications and on-going projects</p>
            </div>
            
            <div class="publications-list">
                <!-- 2026 Publications -->
                <div class="year-section">
                    <div class="year-header">2026</div>
                    <div class="year-publications">
                        <div class="publication-item">
                            <div class="publication-content">
                                <div class="publication-badge cogsci">Preprint</div>
                                <h3 class="paper-title">GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory</h3>
                                <p class="authors">Pepijn Cobben*, Xuanqiang Angelo Huang*, <strong>Thao Amelia Pham*</strong>, Isabel Dahlgren*, Terry Jingchen Zhang, Zhijing Jin</p>
                                <p class="venue"><em>*Equal Contribution. Under Review</em>, Jan 2026</p>
                                <div class="publication-buttons">
                                    <button class="abs-btn" onclick="toggleAbstract(this)">ABS</button>
                                    <a href="" class="arxiv-btn" target="_blank">ArXiv</a>
                                    <a href="" class="pdf-btn" target="_blank">PDF</a>
                                    <a href="" class="LessWrong" target="_blank">LessWrong</a>
                                </div>
                                <div class="abstract-content" style="display: none;">
                                    <p>Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as  coordination failure and conflict poorly understood. We introduce <strong>GT-HarmBench</strong>, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments.</p>
                                </div>
                            </div>
                        </div>
                        <div class="publication-item">
                            <div class="publication-content">
                                <div class="publication-badge cogsci">LaMAS @ AAAI</div>
                                <h3 class="paper-title">Scheming Ability in LLM-to-LLM Strategic Interactions</h3>
                                <p class="authors"><strong>Thao Pham</strong></p>
                                <p class="venue"><em>LaMAS @ AAAI-26</em>, Jan 2026</p>
                                <div class="publication-buttons">
                                    <button class="abs-btn" onclick="toggleAbstract(this)">ABS</button>
                                    <a href="https://arxiv.org/abs/2510.12826" class="arxiv-btn" target="_blank">ArXiv</a>
                                    <a href="https://arxiv.org/pdf/2510.12826" class="pdf-btn" target="_blank">PDF</a>
                                    <a href="../files/LaMAS.pdf" class="poster-btn" target="_blank">Poster</a>
                                </div>
                                <div class="abstract-content" style="display: none;">
                                    <p>As large language model (LLM) agents are deployed autonomously in diverse contexts, evaluating their capacity for strategic deception becomes crucial. While recent research has examined how AI systems scheme against human developers, LLM-to-LLM scheming remains underexplored. We investigate the scheming ability and propensity of frontier LLM agents through two game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro, Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and without explicit prompting while analyzing scheming tactics through chain-of-thought reasoning. When prompted, most models, especially Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance. Critically, models exhibited significant scheming propensity without prompting: all models chose deception over confession in Peer Evaluation (100% rate), while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These findings highlight the need for robust evaluations using high-stakes game-theoretic scenarios in multi-agent settings.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <!-- 2025 Publications -->
                <div class="year-section">
                    <div class="year-header">2025</div>
                        <div class="publication-item">
                            <div class="publication-content">
                                <div class="publication-badge cogsci">CogSci</div>
                                <h3 class="paper-title">Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow</h3>
                                <p class="authors">Kyle Moore, Jesse Roberts, <strong>Thao Pham</strong>, Douglas Fisher</p>
                                <p class="venue"><em>Cognitive Science Society (CogSci)</em>, Apr 2025</p>
                                <div class="publication-buttons">
                                    <button class="abs-btn" onclick="toggleAbstract(this)">ABS</button>
                                    <a href="https://arxiv.org/abs/2408.08651" class="arxiv-btn" target="_blank">ArXiv</a>
                                    <a href="https://escholarship.org/uc/item/18x411vv" class="pdf-btn" target="_blank">PDF</a>
                                </div>
                                <div class="abstract-content" style="display: none;">
                                    <p>Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="margin-top: 2rem;"></div>
                <!-- 2024 Publications -->
                <div class="year-section">
                    <div class="year-header">2024</div>
                    <div class="year-publications">
                        <div class="publication-item">
                            <div class="publication-content">
                                <div class="publication-badge emnlp">EMNLP</div>
                                <h3 class="paper-title">The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies From Benchmark Performance</h3>
                                <p class="authors">Kyle Moore, Jesse Roberts, <strong>Thao Pham</strong>, Oseremhen Ewaleifoh, Douglas Fisher</p>
                                <p class="venue"><em>Empirical Methods in Natural Language Processing (EMNLP)</em>, Sep 2024</p>
                                <div class="publication-buttons">
                                    <button class="abs-btn" onclick="toggleAbstract(this)">ABS</button>
                                    <a href="https://arxiv.org/abs/2406.11634" class="arxiv-btn" target="_blank">ArXiv</a>
                                    <a href="https://arxiv.org/pdf/2406.11634" class="pdf-btn" target="_blank">PDF</a>
                                </div>
                                <div class="abstract-content" style="display: none;">
                                    <p>Cloze testing is a common method for measuring the behavior of large language models on a number of benchmark tasks. Using the MMLU dataset, we show that the base-rate probability (BRP) differences across answer tokens are significant and affect task performance ie. guess A if uncertain. We find that counterfactual prompting does sufficiently mitigate the BRP effect. The BRP effect is found to have a similar effect to test taking strategies employed by humans leading to the conflation of task performance and test-taking ability. We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to disambiguate test-taking ability from task performance and reports the latter.</p>
                                </div>
                            </div>
                        </div>

                        <div class="publication-item">
                            <div class="publication-content">
                                <div class="publication-badge conll">CoNLL</div>
                                <h3 class="paper-title">Large Language Model Recall Uncertainty is Modulated by the Fan Effect</h3>
                                <p class="authors">Jesse Roberts, Kyle Moore, <strong>Thao Pham</strong>, Oseremhen Ewaleifoh, Douglas Fisher</p>
                                <p class="venue"><em>Computational Natural Language Learning (CoNLL)</em>, Sep 2024</p>
                                <div class="publication-buttons">
                                    <button class="abs-btn" onclick="toggleAbstract(this)">ABS</button>
                                    <a href="https://arxiv.org/html/2407.06349" class="arxiv-btn" target="_blank">ArXiv</a>
                                    <a href="https://arxiv.org/pdf/2407.06349v2" class="pdf-btn" target="_blank">PDF</a>
                                </div>
                                <div class="abstract-content" style="display: none;">
                                    <p>Cloze testing is a common method for measuring the behavior of large language models on a number of benchmark tasks. Using the MMLU dataset, we show that the base-rate probability (BRP) differences across answer tokens are significant and affect task performance ie. guess A if uncertain. We find that counterfactual prompting does sufficiently mitigate the BRP effect. The BRP effect is found to have a similar effect to test taking strategies employed by humans leading to the conflation of task performance and test-taking ability. We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to disambiguate test-taking ability from task performance and reports the latter.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </main>
        
    </div>
    
    <!-- Footer -->
    <footer class="site-footer">
        <div class="footer-content">
            <p>&copy; Copyright 2026 Thao Pham.</p>
        </div>
    </footer>
    <script>
        // Theme toggle functionality
        const themeToggle = document.getElementById('theme-toggle');
        const body = document.body;
        
        // Check for saved theme or default to light
        const savedTheme = localStorage.getItem('theme') || 'light';
        body.setAttribute('data-theme', savedTheme);
        
        themeToggle.addEventListener('click', () => {
            const currentTheme = body.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            
            body.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        });

        // Abstract toggle functionality
        function toggleAbstract(button) {
            // Find the abstract content - it's now the next sibling of the button container
            const buttonContainer = button.parentElement;
            const abstractContent = buttonContainer.nextElementSibling;
            const isVisible = abstractContent.style.display !== 'none';
            
            if (isVisible) {
                abstractContent.style.display = 'none';
                button.textContent = 'ABS';
            } else {
                abstractContent.style.display = 'block';
                button.textContent = 'HIDE';
            }
        }
    </script>
</body>
</html>